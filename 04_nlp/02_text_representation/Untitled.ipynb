{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dbd70a7-ddf6-41b6-86d4-d7f9f7456761",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Times New Roman'; font-size:18px; color:black;\">\n",
    "<center>\n",
    "\n",
    "# NLP_9 : FastText Embeddings\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "**FastText** is a word embedding technique developed by **Facebook AI Research (FAIR)**.  \n",
    "It is an extension of **Word2Vec**, designed to overcome Word2Vec’s limitation in handling\n",
    "**rare words and out-of-vocabulary (OOV) words**.\n",
    "\n",
    "Unlike Word2Vec, FastText represents a word as a **collection of character n-grams**, rather than a single token.\n",
    "\n",
    "## 2. Motivation Behind FastText\n",
    "\n",
    "Traditional word embedding models like **Word2Vec** treat each word as an atomic unit.\n",
    "As a result:\n",
    "- Rare words get poor embeddings  \n",
    "- OOV words are ignored completely  \n",
    "- Morphological information is lost  \n",
    "\n",
    "FastText solves this by learning embeddings for **subwords (character n-grams)**.\n",
    "\n",
    "## 3. Core Idea of FastText\n",
    "\n",
    "A word is broken into **character n-grams**, and the word embedding is computed as the **sum (or average)** of its n-gram vectors.\n",
    "\n",
    "### Example\n",
    "\n",
    "Word:playing\n",
    "Character n-grams (n = 3):pl, pla, lay, ayi, yin, ing, ng\n",
    "\n",
    "Final word vector:\n",
    "\n",
    "## 4. Model Architecture\n",
    "\n",
    "FastText uses the same architectures as Word2Vec:\n",
    "- **CBOW**\n",
    "- **Skip-Gram**\n",
    "\n",
    "But instead of learning embeddings for whole words only, it learns embeddings for:\n",
    "- Words  \n",
    "- Character n-grams  \n",
    "\n",
    "\n",
    "## 5. Learning Objective (Skip-Gram FastText)\n",
    "\n",
    "Given a **target word**, FastText predicts surrounding context words using subword information.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$P(w_{context} | w_{target}) = \\sum_{g \\in G(w)} \\vec{z}_g $$\n",
    "\n",
    "where:\n",
    "\n",
    "\\begin{aligned}\n",
    "G(w) & = \\text{set of character n-grams of word } w \\\\\n",
    "\\vec{z}_g & = \\text{embedding vector of n-gram } g\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "## 6. Advantages of FastText\n",
    "\n",
    "- Handles **out-of-vocabulary (OOV)** words  \n",
    "- Performs well on **small datasets**  \n",
    "- Captures **morphological structure**  \n",
    "- Better embeddings for **rare words**  \n",
    "- Works well for **morphologically rich languages**\n",
    "\n",
    "\n",
    "## 7. Limitations\n",
    "\n",
    "- Larger vocabulary size  \n",
    "- Higher memory usage compared to Word2Vec  \n",
    "- Slower training time  \n",
    "- Still limited in capturing long-range context compared to transformers  \n",
    "\n",
    "## 8. Comparison with Word2Vec\n",
    "\n",
    "| Feature | Word2Vec | FastText |\n",
    "|-------|---------|----------|\n",
    "| Uses subword information | No | Yes |\n",
    "| Handles OOV words | No | Yes |\n",
    "| Rare word performance | Weak | Strong |\n",
    "| Vocabulary size | Smaller | Larger |\n",
    "| Training speed | Faster | Slower |\n",
    "\n",
    "\n",
    "## 9. Applications of FastText\n",
    "\n",
    "- Text classification  \n",
    "- Sentiment analysis  \n",
    "- Language identification  \n",
    "- Named Entity Recognition (NER)  \n",
    "- Low-resource languages  \n",
    "\n",
    "\n",
    "## 10. Summary\n",
    "\n",
    "- FastText extends Word2Vec using **character n-grams**\n",
    "- Generates embeddings even for **unseen words**\n",
    "- Ideal for **rare words and morphologically complex languages**\n",
    "- Widely used in real-world NLP systems\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be1320-48db-421a-a1f3-0122d645c981",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; font-family: Times New Roman; font-size: 16px;\">\n",
    "<center>\n",
    "\n",
    "# NLP_9 : FastText Embeddings\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "**FastText** is a word embedding technique developed by **Facebook AI Research (FAIR)**.  \n",
    "It is an extension of **Word2Vec**, designed to overcome Word2Vec’s limitation in handling\n",
    "**rare words and out-of-vocabulary (OOV) words**.\n",
    "\n",
    "Unlike Word2Vec, FastText represents a word as a **collection of character n-grams**, rather than a single token.\n",
    "\n",
    "## 2. Motivation Behind FastText\n",
    "\n",
    "Traditional word embedding models like **Word2Vec** treat each word as an atomic unit.\n",
    "As a result:\n",
    "- Rare words get poor embeddings  \n",
    "- OOV words are ignored completely  \n",
    "- Morphological information is lost  \n",
    "\n",
    "FastText solves this by learning embeddings for **subwords (character n-grams)**.\n",
    "\n",
    "## 3. Core Idea of FastText\n",
    "\n",
    "A word is broken into **character n-grams**, and the word embedding is computed as the **sum (or average)** of its n-gram vectors.\n",
    "\n",
    "### Example\n",
    "\n",
    "Word:playing\n",
    "Character n-grams (n = 3):pl, pla, lay, ayi, yin, ing, ng\n",
    "\n",
    "Final word vector:\n",
    "\n",
    "## 4. Model Architecture\n",
    "\n",
    "FastText uses the same architectures as Word2Vec:\n",
    "- **CBOW**\n",
    "- **Skip-Gram**\n",
    "\n",
    "But instead of learning embeddings for whole words only, it learns embeddings for:\n",
    "- Words  \n",
    "- Character n-grams  \n",
    "\n",
    "\n",
    "## 5. Learning Objective (Skip-Gram FastText)\n",
    "\n",
    "Given a **target word**, FastText predicts surrounding context words using subword information.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$P(w_{context} | w_{target}) = \\sum_{g \\in G(w)} \\vec{z}_g $$\n",
    "\n",
    "where:\n",
    "\n",
    "\\begin{aligned}\n",
    "G(w) & = \\text{set of character n-grams of word } w \\\\\n",
    "\\vec{z}_g & = \\text{embedding vector of n-gram } g\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "## 6. Advantages of FastText\n",
    "\n",
    "- Handles **out-of-vocabulary (OOV)** words  \n",
    "- Performs well on **small datasets**  \n",
    "- Captures **morphological structure**  \n",
    "- Better embeddings for **rare words**  \n",
    "- Works well for **morphologically rich languages**\n",
    "\n",
    "\n",
    "## 7. Limitations\n",
    "\n",
    "- Larger vocabulary size  \n",
    "- Higher memory usage compared to Word2Vec  \n",
    "- Slower training time  \n",
    "- Still limited in capturing long-range context compared to transformers  \n",
    "\n",
    "## 8. Comparison with Word2Vec\n",
    "\n",
    "| Feature | Word2Vec | FastText |\n",
    "|-------|---------|----------|\n",
    "| Uses subword information | No | Yes |\n",
    "| Handles OOV words | No | Yes |\n",
    "| Rare word performance | Weak | Strong |\n",
    "| Vocabulary size | Smaller | Larger |\n",
    "| Training speed | Faster | Slower |\n",
    "\n",
    "\n",
    "## 9. Applications of FastText\n",
    "\n",
    "- Text classification  \n",
    "- Sentiment analysis  \n",
    "- Language identification  \n",
    "- Named Entity Recognition (NER)  \n",
    "- Low-resource languages  \n",
    "\n",
    "\n",
    "## 10. Summary\n",
    "\n",
    "- FastText extends Word2Vec using **character n-grams**\n",
    "- Generates embeddings even for **unseen words**\n",
    "- Ideal for **rare words and morphologically complex languages**\n",
    "- Widely used in real-world NLP systemsand out-of-vocabulary words efficiently. This makes it ideal for morphologically rich languages and tasks where rare words are common.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e36aa5-4ccb-4848-bae2-ee4b037ff6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
